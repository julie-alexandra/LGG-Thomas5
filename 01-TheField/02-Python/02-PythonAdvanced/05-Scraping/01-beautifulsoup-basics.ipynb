{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic data collection on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to tackle some nice web pages (HTML), we will discover the XML language which is a good introduction to scraping data on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists a few properties of the XML language.\n",
    "\n",
    "- XML was created to facilitate data exchange between machines and software.\n",
    "\n",
    "- XML is a language that is written using tags.\n",
    "\n",
    "- XML is a W3C recommendation, so it is a technology with strict rules to follow.\n",
    "\n",
    "- XML is intended to be understandable by everyone: people and machines alike.\n",
    "\n",
    "- XML allows us to create our own vocabulary using a set of customizable rules and tags.\n",
    "\n",
    "- XML is also compatible with the web so that data exchanges can be easily carried out over the Internet.\n",
    "\n",
    "- XML is therefore standardized, simple, but above all extensible and configurable so that any type of data can be described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a XML document, which we have saved as `data.xml` in the `assets/` directory.\n",
    "\n",
    "Display its content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<users>\n",
      "    <user data-id=\"101\">\n",
      "        <name>Zorro</name>\n",
      "        <job>Dancer</job>\n",
      "    </user>\n",
      "    <user data-id=\"102\">\n",
      "        <name>Hulk</name>\n",
      "        <job>Football player</job>\n",
      "    </user>\n",
      "    <user data-id=\"103\">\n",
      "        <name>Zidane</name>\n",
      "        <job>Star</job>\n",
      "    </user>\n",
      "    <user data-id=\"104\">\n",
      "        <name>Beans</name>\n",
      "        <job>Grocer</job>\n",
      "    </user>\n",
      "    <user data-id=\"105\">\n",
      "        <name>Batman</name>\n",
      "        <job>Veterinary</job>\n",
      "    </user>\n",
      "    <user data-id=\"106\">\n",
      "        <name>Spiderman</name>\n",
      "        <job>Veterinary</job>\n",
      "    </user>\n",
      "</users>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"./assets/data.xml\"\n",
    "file = open(filename, \"r\")\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line indicates the encoding (we always stay in the UTF-8 encoding). Then we notice that the \"users\" tag has other \"user\" tags that themselves have their own tags. The data is hierarchized in a tree and each node provides information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small script that displays all the usernames.\n",
    "\n",
    "You will first have to install the `lxml` package. It depends on the `numpy` package, which will be installed alongside `lxml` if you use a standard package manager. However, some version of `numpy` give problems, so changing the version might be the first thing that you can troubleshoot if you fail to import `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# I define my source document\n",
    "tree = etree.parse(filename)\n",
    "# I look at my document and identify the path to the tag to get to the \"user\" information\n",
    "\n",
    "# The user name we are looking for is in it's own tag, `name`. Which itself\n",
    "# is in a tag `user`, and lastly `user` is contained in a `users` tag.\n",
    "# So tree.xpath(\"/users/user/name\") contains the tags associated with our search\n",
    "for user in tree.xpath(\"/users/user/name\"):\n",
    "    # I only want to display the content (.text) of the `/users/user/name` tags\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.xpath(\"/users/user/name\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display the attributes of the tags that store this information\n",
    "tree = etree.parse(filename)\n",
    "for user in tree.xpath(\"/users/user\"):\n",
    "    print(user.get(\"data-id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refine the display by proposing to display only users whose job is Veterinary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(filename)\n",
    "# Quel joli petit dictionnaire\n",
    "for user in tree.xpath(\"/users/user[job='Veterinary']/name\"):\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Web scraping is a technique for automatically extracting information from websites. It is very useful in a lot of use cases:\n",
    "\n",
    "- **E-commerce**: create Excel sheet gathering all products from a provider's website\n",
    "- **Business prospecting**: extract contact information from websites (phone numbers, e-mail addresses...)\n",
    "- **Media analysis**: collect articles from online newspapers on a daily basis\n",
    "- ...\n",
    "\n",
    "Of course this task could be done by a human but this would be very painful and repetitive especially when the number of targeted pages is huge. To save time, we can automate it and, as usual, Python has a solution for that.\n",
    "\n",
    "The idea is to automate the human task by **fetching** pages and **extracting** data from them.\n",
    "\n",
    "**Fetching**\n",
    "\n",
    "Fetching is the downloading of the `html` source code of a page. It's exactly what your browser does when you open a website. To test it, open your browser, right-click anywhere on the page and select `view page source`. This is what Python will automate !\n",
    "\n",
    "**Extraction**\n",
    "\n",
    "From the `html` page you can extract, transform and load information in Python exactly like we did before with `xml`.\n",
    "\n",
    "Let's implement those two steps in few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching: Scraping via HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP (HyperText Transfer Protocol) is a protocol that will allow a **client** (you, through your browser or your code for example) to communicate with a **server** connected to the network (hosting a website or an internet document)\n",
    "\n",
    "Requests always go in pairs: the request (from the client) and the response (from the server).\n",
    "If this is not the case, it is because a problem has occurred at a point in the network.\n",
    "\n",
    "In Python we can use the library `requests` with the method `get`. It will emulate what you are doing manually in your browser.\n",
    "\n",
    "Start by installing the library using `pip install requests` or the `conda` package manager.\n",
    "\n",
    "Then let's take an URL, download the content and display it to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.becode.org/about/ 200\n",
      "b'<!DOCTYPE html>\\n<html lang=\"en-US\">\\n<head><style>img.lazy{min-height:1px}</style><link rel=\"preload\" href=\"https://becode.org/wp-content/plugins/w3-total-cache/pub/js/lazyload.min.js\" as=\"script\">\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, viewport-fit=cover\" />\\t\\t<script data-cookieconsent=\"ignore\">\\n\\twindow.dataLayer = window.dataLayer || [];\\n\\tfunction gtag() {\\n\\t\\tdataLayer.push(arguments);\\n\\t}\\n\\tgtag(\"consent\", \"default\", {\\n\\t\\tad_user_data: \"denied\",\\n\\t\\tad_personalization: \"denied\",\\n\\t\\tad_storage: \"denied\",\\n\\t\\tanalytics_storage: \"denied\",\\n\\t\\tfunctionality_storage: \"denied\",\\n\\t\\tpersonalization_storage: \"denied\",\\n\\t\\tsecurity_storage: \"granted\",\\n\\t\\twait_for_update: 500,\\n\\t});\\n\\tgtag(\"set\", \"ads_data_redaction\", true);\\n\\tgtag(\"set\", \"url_passthrough\", true);\\n</script>\\n<script type=\"text/javascript\"\\n\\t\\tid=\"Cookiebot\"\\n\\t\\tsrc=\"https://consent.cookiebot.com/uc.js\"\\n\\t\\tdata-cbid=\"50896b4e-b545-4015-9958-a06abe5021f6\"\\n\\t\\t\\t\\t\\t\\t\\tdata-blockingmode=\"auto\"\\n\\t></script>'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Url of website\n",
    "url = \"https://www.becode.org/about/\"\n",
    "# I send my HTTP request with a \"GET\" to the site server to identify in the url\n",
    "r = requests.get(url)\n",
    "# I display the requested url and the return of the server\n",
    "print(url, r.status_code)\n",
    "# I will store the content of the website in a variable and print it\n",
    "content = r.content\n",
    "print(content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the content is not easily readable. But like `xml`, `html` is a structured representation of the content. We then can use Python for reading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction using Beautifulsoup\n",
    "\n",
    "For parsing `xml`we have used `xpath`. This is also possible for `html` ([see here](https://python-docs.readthedocs.io/en/latest/scenarios/scrape.html)).\n",
    "\n",
    "However there is a more _user-friendly_ library in Python that does the job quite well: `beautifulsoup`. We will use it in this notebook.\n",
    "\n",
    "First install it using `pip install beautifulsoup4` or the `conda` package manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the website content we just scraped in a Beautifulsoup object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(content, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `html` tree of the page is now loaded and we can access its information. For example we can get the title of the page by recovering the content of the tag `h1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passion for learning.\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"h1\"):\n",
    "    # We only retrieve the content ==> .text\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with `h2` tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeCode’s pedagogical framework\n",
      "Our goal\n",
      "We deliver!\n",
      "<span data-metadata=\"\"><span data-buffer=\"\">Our Team\n",
      "How it started...\n",
      "Contact us\n",
      "Job seekers\n",
      "Companies\n",
      "BeCode\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"h2\"):\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, do the same with the `p` tags (that stand for _paragraphs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "\n",
      " \n",
      "Get trained\n",
      "Learn\n",
      "All trainings\n",
      "Long trainings\n",
      "AI & Data science\n",
      "Junior Developer\n",
      "Junior Software Developer\n",
      "Microsoft Dynamics 365\n",
      "Cyber Security\n",
      "Cisco Network Engineer\n",
      "SAP Consultant\n",
      "Tech hubs For All\n",
      "Short trainings\n",
      "AI Sprint pretraining\n",
      "Cyber Sprint pretraining\n",
      "Starter Sprint\n",
      "One Week Trainings\n",
      "E-learning\n",
      "No-Code online training\n",
      "Our Campuses\n",
      "BeCode Brussels\n",
      "BeCode Charleroi\n",
      "BeCode Ghent\n",
      "BeCode Liège\n",
      "BeCode Talents\n",
      "Pedagogy\n",
      "Events\n",
      "FAQ\n",
      "About us\n",
      "About us\n",
      "Team\n",
      "Values\n",
      "Press Reviews\n",
      "Impact\n",
      "Jobs\n",
      "Projects\n",
      "Women Mentoring Program\n",
      "Digibanken\n",
      "Tech Time 2 Skill\n",
      "Capac’IT 4 Africa\n",
      "Femme Forward\n",
      "Companies\n",
      "Partnerships\n",
      "Our promoters\n",
      "Become a partner\n",
      "Meet the team\n",
      "Train talents\n",
      "AI as a productivity booster\n",
      "CyberSec in 1 day\n",
      "Tailor-made training\n",
      "Hire talents\n",
      "Staff Talents\n",
      "Tech hub\n",
      "Partner Day\n",
      "Events\n",
      "How you can help\n",
      "Skills Sponsorship\n",
      "DONATE\n",
      "English\n",
      "Nederlands\n",
      "Français\n",
      "Get trained\n",
      "Learn\n",
      "All trainings\n",
      "Long trainings\n",
      "AI & Data science\n",
      "Junior Developer\n",
      "Junior Software Developer\n",
      "Microsoft Dynamics 365\n",
      "Cyber Security\n",
      "Cisco Network Engineer\n",
      "SAP Consultant\n",
      "Tech hubs For All\n",
      "Short trainings\n",
      "AI Sprint pretraining\n",
      "Cyber Sprint pretraining\n",
      "Starter Sprint\n",
      "One Week Trainings\n",
      "E-learning\n",
      "No-Code online training\n",
      "Our Campuses\n",
      "BeCode Brussels\n",
      "BeCode Charleroi\n",
      "BeCode Ghent\n",
      "BeCode Liège\n",
      "BeCode Talents\n",
      "Pedagogy\n",
      "Events\n",
      "FAQ\n",
      "About us\n",
      "About us\n",
      "Team\n",
      "Values\n",
      "Press Reviews\n",
      "Impact\n",
      "Jobs\n",
      "Projects\n",
      "Women Mentoring Program\n",
      "Digibanken\n",
      "Tech Time 2 Skill\n",
      "Capac’IT 4 Africa\n",
      "Femme Forward\n",
      "Companies\n",
      "Partnerships\n",
      "Our promoters\n",
      "Become a partner\n",
      "Meet the team\n",
      "Train talents\n",
      "AI as a productivity booster\n",
      "CyberSec in 1 day\n",
      "Tailor-made training\n",
      "Hire talents\n",
      "Staff Talents\n",
      "Tech hub\n",
      "Partner Day\n",
      "Events\n",
      "How you can help\n",
      "Skills Sponsorship\n",
      "DONATE\n",
      "English\n",
      "Nederlands\n",
      "Français\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Explore BeCode Charleroi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Let's meet!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Discover the team\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Discover Simplon\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "All trainings\n",
      "Campuses\n",
      "All trainings\n",
      "Campuses\n",
      "Hire Talents\n",
      "Partnership options\n",
      "Donate\n",
      "Hire Talents\n",
      "Partnership options\n",
      "Donate\n",
      "Campuses\n",
      "Values\n",
      "Team\n",
      "Activity Report FR – 2023\n",
      "Activity Report NL – 2023\n",
      "Campuses\n",
      "Values\n",
      "Team\n",
      "Activity Report FR – 2023\n",
      "Activity Report NL – 2023\n",
      "Terms\n",
      "Privacy\n",
      "Cookies\n",
      "\n",
      "English\n",
      "\n",
      "Nederlands\n",
      "\n",
      "Français\n"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"a\"):\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Choose an URL, get its content by using `requests`. Load it by using `beautifulsoup`. Then create a summary of the page by printing the content of `h1`, the first `h2` and the first paragraph (`p`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.grignoux.be/fr/ 200\n",
      "Else\n",
      "Love Streams\n",
      "Mexico 86\n",
      "Natacha (presque) hôtesse de l'air\n",
      "Travail soigné\n",
      "Mexico 86\n",
      "Nos films à l'affiche\n",
      "Warfare\n",
      "Deux soeurs (Hard Truths)\n",
      "Les Damnés\n",
      "Mickey 17\n",
      "Sarah Bernhardt, la divine\n",
      "Soft Leaves\n",
      "Blue Velvet\n",
      "Caught by the Tides\n",
      "The Outrun\n",
      "Diamant Brut\n",
      "Flow, le chat qui n'avait plus peur de l'eau\n",
      "The Seed of The Sacred Fig\n",
      "À bicyclette !\n",
      "Black Bag (The Insider)\n",
      "Fuga\n",
      "Ma mère, Dieu et Sylvie Vartan\n",
      "L'Attachement\n",
      "Mémoires d'un escargot\n",
      "Merckx\n",
      "On ira\n",
      "Parthenope\n",
      "Prosper\n",
      "Aimer Perdre\n",
      "Vermiglio\n",
      "Else\n",
      "Namur 2030\n",
      "Prochaine mise en ligne des séances : vendredi 11 avril 2025\n",
      "Nos événements\n",
      "Focus Prix Lux\n",
      "Focus Prix Lux\n",
      "Namur 2030 - Candidate capitale européenne de la culture\n",
      "Else\n",
      "Exploration du monde : Guyane - Suriname\n",
      "Love Streams\n",
      "Mexico 86\n",
      "Natacha (presque) hôtesse de l'air\n",
      "Travail soigné\n",
      "Exploration du monde : Guyane - Suriname\n",
      "Mexico 86\n",
      "The Outrun\n",
      "Motel Destino\n",
      "Oasis\n",
      "Racines\n",
      "Paddington au Pérou\n",
      "Ciné-Seniors : À bicyclette !\n",
      "Racines\n",
      "Fast Fashion, les dessous de la mode à bas prix\n",
      "Le Mélange des genres\n",
      "Un p'tit truc en plus\n",
      "Reflet dans un diamant mort\n",
      "Reflet dans un diamant mort\n",
      "Memories of Murder\n",
      "Sister Act VF\n",
      "Let's Get Lost\n",
      "Notre Kanaal\n",
      "Jazz à Liège\n",
      "Fête du tram : (Enfin) au coeur de Liège !\n",
      "Croquantes\n",
      "Actualités\n"
     ]
    }
   ],
   "source": [
    "url2 = \"https://www.grignoux.be/fr/\"\n",
    "r2 = requests.get(url2)\n",
    "print(url2, r2.status_code)\n",
    "content2 = r2.content \n",
    "\n",
    "\n",
    "soup2 = BeautifulSoup(content2, \"html\")\n",
    "\n",
    "for tag2 in soup2.find_all(\"h2\"): \n",
    "    print(tag2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you did it ! Let's go with more advanced operation using `beautifulsoup`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
