{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df04b7b",
   "metadata": {},
   "source": [
    "# Limitations with Large Language Models\n",
    "\n",
    "Large Language Models are objectively great. They are flexible, surprisingly cunning, and have a considerable amount of knowledge by themselves. They do come short in some cases, especially when it comes to adapting to new contextual information. Let's say you're trying to build an LLM that answers all the questions you may have about BeCode rules. What does ChatGPT know about BeCode rules, was there any of it in its training data? Probably not much.\n",
    "\n",
    "## How could we have a LLM answer BeCode questions?\n",
    "\n",
    "An LLM has a very long context window, close to 1 million words for ChatGPT, that means close to two books from Game of Thrones can be given to it and it would still be able to answer. We could give it all of BeCode rules as a text in the prompt and have it answer questions based on them. But it still comes with many caveats, mostly that giving a lot of content to an LLM is quite costly in resources and money.\n",
    "\n",
    "Wouldn't it be better if we could just give it the parts of the document useful in the answer to help it on the prompt at hand? The LLM doesn't need to be told about the way moodle works in order to explain when the holidays of the bootcamp happen. The document with Becode Rules is given in `data/becode_rules.txt`.\n",
    "\n",
    "Make it so that the LLM can answer the following prompt by giving it the paragraph from becode_rules that will allow it to answer the prompt, insert this in the code snippet underneath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a098d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Python SDK\n",
    "from google import genai\n",
    "\n",
    "API_KEY=\"GEMINI API KEY HERE\"\n",
    "\n",
    "if API_KEY==\"GEMINI API KEY HERE\":\n",
    "    raise Exception('Your API key has to be put instead of \"GEMINI API KEY HERE\"')\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)#Telling google what your API key is\n",
    "\n",
    "question = 'I am sick, I sent an email to Antoine and Cindy, what else should I do?'\n",
    "context = '''PASTE INFORMATION IN THE DOCUMENT ABOUT BEING SICK'''\n",
    "prompt = f'Use the following snippet:\\n {context}\\n\\n To answer this question: {question}'\n",
    "print(\"Prompt:\\n\",prompt)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=prompt\n",
    ")\n",
    "print(\"\\n\\nAnswer:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361b266",
   "metadata": {},
   "source": [
    "## Word2Vec makes a comeback\n",
    "\n",
    "You just had to give the answer to ChatGPT to have it tell you what to do. Not very handy, might as well go search in the document yourself. But what if there was a way to make a program perform that search automatically?\n",
    "\n",
    "In the previous notebooks, you may have read that we used to turn some words into vectors to encode meaning about them, and that words with similar meanings had similar vectors. Well what if you could do this instead with many words? What if you could do it with paragraphs? Wouldn't that be great.\n",
    "\n",
    "Well as it turns out, you can, you can make [embeddings for paragraphs](https://ai.google.dev/gemini-api/docs/embeddings). You can do that with an entire document, and then use the paragraph who's vectors are similar to your prompt to augment it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embed = client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=\"I am sick, I sent an email to Antoine and Cindy, what else should I do?\")\n",
    "\n",
    "print(prompt_embed.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content=\"\"\n",
    "with open('data/becode_rules.txt','r',encoding=\"utf-8\") as f:\n",
    "    doc_content=f.read()\n",
    "\n",
    "chunk_size=500\n",
    "chunks=[]\n",
    "nb_full_chunks=len(doc_content)//chunk_size\n",
    "\n",
    "for i in range(nb_full_chunks):\n",
    "    chunks.append(doc_content[i*chunk_size:(i+1)*chunk_size])\n",
    "last_chunk_start=nb_full_chunks*chunk_size\n",
    "chunks.append(doc_content[last_chunk_start:last_chunk_start+len(doc_content)%chunk_size])\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf28ea",
   "metadata": {},
   "source": [
    "Run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15713394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Prompt embedding\n",
    "prompt_vector = np.array(prompt_embed.embeddings[0].values).reshape(1, -1)\n",
    "\n",
    "# Chunk 3 and 4 embeddings\n",
    "chunk_3_vector = np.array(\n",
    "    client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=chunks[3]\n",
    "    ).embeddings[0].values\n",
    ").reshape(1, -1)\n",
    "\n",
    "chunk_4_vector = np.array(\n",
    "    client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=chunks[4]\n",
    "    ).embeddings[0].values\n",
    ").reshape(1, -1)\n",
    "\n",
    "# Print the text\n",
    "print(\"Chunk 3:\")\n",
    "print(chunks[3])\n",
    "\n",
    "print(\"\\nChunk 4:\")\n",
    "print(chunks[4])\n",
    "\n",
    "# Compute cosine similarities\n",
    "sim_3 = cosine_similarity(prompt_vector, chunk_3_vector)[0][0]\n",
    "sim_4 = cosine_similarity(prompt_vector, chunk_4_vector)[0][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSimilarity between the prompt and Chunk 3: {sim_3:.4f}\")\n",
    "print(f\"Similarity between the prompt and Chunk 4: {sim_4:.4f}\")\n",
    "\n",
    "print(\"Is this giving you any ideas????\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b1cd4",
   "metadata": {},
   "source": [
    "Take time to understand the example given above. Then try to make a system which can answer any question by selecting the best chunks from the content to answer the prompt. You may explore different chunk sizes, some overlap between chunks and a criterion for minimum required similarity. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eba503",
   "metadata": {},
   "source": [
    "## Where is this going\n",
    "\n",
    "I hope you'll quickly realise how powerful this approach can be when augmenting the prompts of LLMS. You just need to find the segments with the highest similarity and feed them in the prompt. This is the beauty of Retrieval Augmented Generation (RAG). There are many things to explore from here and I invite you to go look for these topics in whichever way you prefer. Here is an example list of ideas:\n",
    "\n",
    "- You can look into different embedding models for sentences and paragraphs. This is the current overall [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "- You can look into ways google has of using sentence embeddings which may be a little less clunky than what we did above\n",
    "- You can have a peek into ways of augmenting RAG with a classic keywords search\n",
    "- You can try to replicate this example with something you care about\n",
    "\n",
    "Have fun with it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
